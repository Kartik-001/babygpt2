{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "aCoyeLjqDOId"
   },
   "outputs": [],
   "source": [
    "# !pip install torch numpy tqdm wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7EcLbcvKjqo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdL7jLKmEpKk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdCubNf5qPuc"
   },
   "source": [
    "## Phase 1: Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gBQ9DyGqQQS"
   },
   "outputs": [],
   "source": [
    "vocab_size   = 50257    # matches our tokenizer\n",
    "block_size   = 128      # context window (you can grow later)\n",
    "d_model      = 256      # model â€œwidthâ€ (embedding size)\n",
    "n_heads      = 4        # number of attention heads\n",
    "n_layers     = 4        # number of transformer blocks\n",
    "dropout_rate = 0.1      # small dropout to regularize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdFGhXvDqvOr"
   },
   "source": [
    "## ğŸš§ Phase 2: Core Components\n",
    "\n",
    "Weâ€™ll implement each in PyTorch:\n",
    "\n",
    "1. **Token + Positional Embeddings**\n",
    "    - `nn.Embedding(vocab_size, d_model)`\n",
    "    - `nn.Embedding(block_size, d_model)`\n",
    "    - Sum them.\n",
    "2. **Multiâ€‘Head Selfâ€‘Attention**\n",
    "    - Linear projections for Q, K, V: each `d_model â†’ d_model`\n",
    "    - Split into `n_heads` (i.e. reshape to `(batch, heads, seq, d_head)`)\n",
    "    - Scaled dotâ€‘product with causal mask\n",
    "    - Concat heads â†’ final projection\n",
    "3. **Feedâ€‘Forward Network (FFN)**\n",
    "    - Two linear layers:\n",
    "        - `d_model â†’ 4Â·d_model`\n",
    "        - Activation (GELU)\n",
    "        - `4Â·d_model â†’ d_model`\n",
    "4. **Transformer Block**\n",
    "    - Preâ€‘LayerNorm\n",
    "    - Attention + residual\n",
    "    - Preâ€‘LayerNorm\n",
    "    - FFN + residual\n",
    "5. **Stack of Blocks + Final LayerNorm**\n",
    "    - Repeat `n_layers`\n",
    "    \n",
    "    - Final `LayerNorm(d_model)`\n",
    "6. **Output Head (LM Head)**\n",
    "    - Tie weights with token embedding: project final hidden state back to `vocab_size` logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMr6cIECrDRE"
   },
   "source": [
    "1. Token + Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2eWVH3xqYy7"
   },
   "outputs": [],
   "source": [
    "class TokenPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed   = nn.Embedding(block_size, d_model)\n",
    "        self.dropout     = nn.Dropout(0.1)  # GPT-2 uses dropout after embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()  # (batch_size, sequence_length)\n",
    "        # sanity check\n",
    "        assert T <= self.pos_embed.num_embeddings, (\n",
    "            f\"Sequence length T={T} exceeds block_size={self.pos_embed.num_embeddings}\"\n",
    "        )\n",
    "        tok_emb = self.token_embed(x)                    # (B, T, d_model)\n",
    "        pos_ids = torch.arange(T, device=x.device)       # (T,)\n",
    "        pos_emb = self.pos_embed(pos_ids)[None, :, :]    # (1, T, d_model)\n",
    "        out = tok_emb + pos_emb                          # (B, T, d_model)\n",
    "        return self.dropout(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG4w0qJ-r2gz"
   },
   "source": [
    "Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrYh80B_r3As"
   },
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "d_model = 256\n",
    "block_size = 128\n",
    "\n",
    "embed_layer = TokenPositionalEmbedding(vocab_size, d_model, block_size)\n",
    "\n",
    "dummy_input = torch.randint(0, vocab_size, (4, block_size))  # (batch=4)\n",
    "output = embed_layer(dummy_input)\n",
    "\n",
    "print(output.shape)  # â†’ torch.Size([4, 128, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyDkkUy37vsC"
   },
   "source": [
    "2. Multiâ€‘Head Selfâ€‘Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPNX9wZWr5FJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        # Linear projections for queries, keys, values, and final output\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection (projects concatenated heads back to d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # The mask will be created in forward based on input shape\n",
    "        # self.register_buffer(\n",
    "        #     \"mask\",\n",
    "        #     torch.tril(torch.ones((1, 1, 512, 512), dtype=torch.bool)),  # 512 = max block_size; adjust if you use different\n",
    "        #     persistent=False\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, d_model)  batch of embeddings\n",
    "        returns: (B, T, d_model) same shape, after self-attention\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()  # C = d_model\n",
    "        assert C == self.n_heads * self.d_head\n",
    "\n",
    "        # 1. project to queries, keys, values and reshape for multi-head\n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # (B, nh, T, dh)\n",
    "        k = self.k_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # (B, nh, T, dh)\n",
    "        v = self.v_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # (B, nh, T, dh)\n",
    "\n",
    "        # 2. compute scaled dot-product attention scores\n",
    "        #    q @ k^T : (B, nh, T, dh) @ (B, nh, dh, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "\n",
    "        # 3. apply causal mask: prevent attending to future positions\n",
    "        #    Create mask based on current sequence length T\n",
    "        causal_mask = torch.tril(torch.ones((T, T), device=x.device, dtype=torch.bool))[None, None, :, :] # (1, 1, T, T)\n",
    "        att = att.masked_fill(~causal_mask, float('-inf'))\n",
    "\n",
    "        # 4. softmax and dropout\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        # 5. attention output weighted sum\n",
    "        #    (B, nh, T, T) @ (B, nh, T, dh) -> (B, nh, T, dh)\n",
    "        out = att @ v\n",
    "\n",
    "        # 6. combine heads and final projection\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # (B, T, d_model)\n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olV_QSaaPxYI"
   },
   "outputs": [],
   "source": [
    "# Suppose d_model=12 and n_heads=3 (so d_head=4) for a toy example\n",
    "d_model = 12\n",
    "n_heads = 3\n",
    "d_head = d_model // n_heads\n",
    "\n",
    "# Create a dummy q_proj just to inspect its weight\n",
    "q_proj = nn.Linear(d_model, d_model)\n",
    "W = q_proj.weight.data  # shape: (12, 12)\n",
    "\n",
    "# Split W into 3 heads of size 4Ã—12 each\n",
    "heads = W.view(n_heads, d_head, d_model)\n",
    "\n",
    "for i, W_i in enumerate(heads):\n",
    "    print(f\"Head {i} weight shape: {W_i.shape}\")\n",
    "    # This W_i is exactly the 4Ã—12 matrix that each head uses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f40567bf"
   },
   "source": [
    "3. Feedâ€‘Forward Network (FFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae9e3069"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fZY3W8FRkDJ"
   },
   "source": [
    "4. Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMqBqt8W8GXj"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, dropout_rate=0.1): # Added dropout_rate here\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, dropout_rate) # Pass dropout_rate\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, dropout_rate) # Pass dropout_rate\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SR_s9W4AQhuz"
   },
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, d_model, n_heads, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 1) Embeddings\n",
    "        self.token_pos_embed = TokenPositionalEmbedding(vocab_size, d_model, block_size)\n",
    "\n",
    "        # 2) Stacked Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, block_size, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # 3) Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 4) LM head (tie weights to token embedding)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_pos_embed.token_embed.weight\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        idx: (B, T) token IDs\n",
    "        returns:\n",
    "          logits: (B, T, vocab_size)\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        x = self.token_pos_embed(idx)  # (B, T, d_model)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)               # (B, T, d_model)\n",
    "\n",
    "        # Final norm\n",
    "        x = self.ln_f(x)              # (B, T, d_model)\n",
    "\n",
    "        # LM head to vocab logits\n",
    "        logits = self.lm_head(x)      # (B, T, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MT29Ux1JB1jw"
   },
   "source": [
    "## Phase 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNV5dGF3CdRm"
   },
   "outputs": [],
   "source": [
    "# In Colab\n",
    "!wget -O tiny_shakespeare.txt \\\n",
    "     https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAyVgzFQCQlQ"
   },
   "outputs": [],
   "source": [
    "# In a Colab code cell\n",
    "import glob\n",
    "\n",
    "# Read the entire file into one big string\n",
    "with open(\"tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "print(f\"Loaded {len(text_data):,} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7fsvYc88Cdy"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMVwbPbRQbpR"
   },
   "outputs": [],
   "source": [
    "# pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3j56c8yQeu5"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello world! How are you?\"\n",
    "\n",
    "# Encode to token IDs\n",
    "token_ids = enc.encode(text)\n",
    "print(token_ids)\n",
    "\n",
    "# Decode back\n",
    "decoded_text = enc.decode(token_ids)\n",
    "print(decoded_text)\n",
    "\n",
    "# Special tokens are not included by default in GPT-2 encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9zcwZPKQiuR"
   },
   "outputs": [],
   "source": [
    "print(\"Vocab size:\", enc.n_vocab)  # 50257 (GPT-2 has 50256 regular + 1 special)\n",
    "print(\"Tokens:\", enc.encode(\" endoftext\"))  # Usually [50256]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fs3PVk82D1E0"
   },
   "source": [
    "1. Prepared the training text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYPni2R5XMSJ"
   },
   "source": [
    "Tokenize your full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldI8_13tGOVQ"
   },
   "outputs": [],
   "source": [
    "tokens = enc.encode(text_data)\n",
    "print(f\"Total tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVrFI--pSfNy"
   },
   "source": [
    " 1: Save or Cache the Tokenized Data (Optional but recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hpcv2yHlSe3m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tokens_np = np.array(tokens, dtype=np.uint16)  # or uint32 if vocab is larger\n",
    "np.save(\"tokens.npy\", tokens_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3mcFlAJSkCz"
   },
   "outputs": [],
   "source": [
    "tokens = np.load(\"tokens.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CA7pP2gDL1Et"
   },
   "source": [
    "2: Split into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20d49c59"
   },
   "outputs": [],
   "source": [
    "split_ratio = 0.9  # 90% train, 10% validation\n",
    "n = int(len(tokens) * split_ratio)\n",
    "train_tokens = tokens[:n]\n",
    "val_tokens = tokens[n:]\n",
    "print(f\"Train: {len(train_tokens)} tokens, Val: {len(val_tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMpcCi4xW8r4"
   },
   "source": [
    "3: Define a Dataset Class (for PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45h0-PTMTAmi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GPTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = torch.tensor(data, dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.block_size]\n",
    "        y = self.data[idx + 1 : idx + 1 + self.block_size]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDkOghnRTDzz"
   },
   "source": [
    "4: Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voyPtm5BTFIz"
   },
   "outputs": [],
   "source": [
    "block_size = 128  # or 256, 512, depending on memory\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = GPTDataset(train_tokens, block_size)\n",
    "val_dataset = GPTDataset(val_tokens, block_size)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXWWAnDHTexL"
   },
   "source": [
    "5: Define the GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "915f5ba3"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Z1TupQ6Tj0S"
   },
   "outputs": [],
   "source": [
    "model = GPT2Model(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=128,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    dropout=0.1\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4O0vbg_T82e"
   },
   "source": [
    " 6: Set up the Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wt-HvUsvT-oh"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zDJ_DNPUI1U"
   },
   "source": [
    "7: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQ8bXqhYUK7C"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device) # Move input to the same device as the model\n",
    "        y = y.to(device) # Move target to the same device as the model\n",
    "        logits = model(x)  # (B, T, vocab_size)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b69c42d"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Use tqdm for a progress bar for the training loader\n",
    "    from tqdm.auto import tqdm\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\")\n",
    "\n",
    "    for x, y in train_loop:\n",
    "        x = x.to(device)  # Move input to the same device as the model\n",
    "        y = y.to(device)  # Move target to the same device as the model\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # Calculate loss\n",
    "        # Reshape logits and y to (batch_size * sequence_length, vocab_size) and (batch_size * sequence_length,)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update model parameters\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Optional: Add validation loop here to evaluate on validation set\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\")\n",
    "        for x, y in val_loop:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "            val_loop.set_postfix(loss=loss.item())\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Optional: Save model checkpoint\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pt\")\n",
    "\n",
    "print(\"\\nTraining finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2IXOixsUe1s"
   },
   "source": [
    "8: Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6g570V0Ukmq"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"babygpt2_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lcyjxovW-E2"
   },
   "source": [
    "Let's run a command to list the files in the current directory to see if gpt2_model.pt is present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBHn2ScWW4Tm"
   },
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-fIz2KPUnba"
   },
   "source": [
    "9: Inference (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAP3ADsKUoty"
   },
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx)\n",
    "        logits = logits[:, -1, :]  # only last token\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f319a6b"
   },
   "outputs": [],
   "source": [
    "# Make sure the model is on the correct device\n",
    "model.to(device)\n",
    "\n",
    "# 1. Define a prompt\n",
    "prompt = \"To be or not to be, that is the question:\"\n",
    "\n",
    "# 2. Encode the prompt\n",
    "# Ensure 'enc' (your tokenizer) is available in the notebook\n",
    "token_ids = enc.encode(prompt)\n",
    "\n",
    "# 3. Convert to tensor and move to device\n",
    "x = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "\n",
    "# 4. Generate text (e.g., 50 new tokens)\n",
    "max_new_tokens = 50\n",
    "generated_tokens = generate(model, x, max_new_tokens)\n",
    "\n",
    "# 5. Decode the generated tokens\n",
    "# Convert the tensor back to a list of IDs before decoding\n",
    "generated_text = enc.decode(generated_tokens[0].tolist())\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9OTVaZkiiFT"
   },
   "source": [
    "So, while the foundational coding work is largely complete, the training and evaluation phases are essential for having a practical, working language model. You've built the engine and the car, but it still needs to be driven and tuned to perform well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_0yRGUHhhzE"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18f3f187"
   },
   "source": [
    "## adding wandb logging, early stopping, resume from checkpoint, etc.\n",
    "\n",
    "All those extrasâ€”Weights & Biases logging, early stopping, checkpointing, schedulers, gradient clippingâ€”might look like â€œboilerplate,â€ but each serves a clear purpose in real-world model training:\n",
    "\n",
    "this block of code is your enhanced training pipeline. While you already have a basic loop that updates model weights, this version adds the following to help you train more effectively and manage experiments:\n",
    "\n",
    "**Experiment Tracking (wandb):**\n",
    "\n",
    "*   Logs your hyperparameters, training/validation losses, learning rates, and other metrics in real time.\n",
    "*   Makes it trivial to compare runs, visualize curves, and share results with collaborators.\n",
    "\n",
    "**Checkpointing & Resume:**\n",
    "\n",
    "*   Saves the â€œbestâ€ model so far (lowest validation loss) to disk.\n",
    "*   If your notebook or Colab crashesâ€”or you simply want to stop and come backâ€”you can resume training exactly where you left off, without losing days of work.\n",
    "\n",
    "**Early Stopping:**\n",
    "\n",
    "*   Stops training when validation loss hasnâ€™t improved for a set number of epochs (patience).\n",
    "*   Prevents over-training (overfitting) and saves compute/energy by not wasting epochs once the model has plateaued.\n",
    "\n",
    "**Learning Rate Scheduler (ReduceLROnPlateau):**\n",
    "\n",
    "*   Automatically reduces the learning rate when validation loss stops improving.\n",
    "*   Helps the optimizer make finer adjustments later in training, often leading to lower final loss.\n",
    "\n",
    "**Gradient Clipping:**\n",
    "\n",
    "*   Caps the gradient norm to avoid â€œexploding gradientsâ€ which can destabilize training, especially in deep or large-step models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NysTN7twhjCk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "# â”€â”€â”€ 1) CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "config = {\n",
    "    \"vocab_size\":     vocab_size,    # e.g. 50257\n",
    "    \"block_size\":     block_size,    # e.g. 128\n",
    "    \"d_model\":        d_model,       # e.g. 256\n",
    "    \"n_heads\":        n_heads,       # e.g. 4\n",
    "    \"n_layers\":       n_layers,      # e.g. 4\n",
    "    \"dropout\":        dropout_rate,  # e.g. 0.1\n",
    "    \"learning_rate\":  3e-4,\n",
    "    \"batch_size\":     32,\n",
    "    \"epochs\":         20,\n",
    "    \"patience\":       3,             # for early stopping\n",
    "    \"checkpoint_dir\": \"./checkpoints\",\n",
    "    \"project_name\":   \"baby-gpt2\",\n",
    "}\n",
    "os.makedirs(config[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€ 2) WANDB SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "wandb.init(project=config[\"project_name\"], config=config)\n",
    "wandb.watch_called = False  # ensure watch only once\n",
    "\n",
    "# â”€â”€â”€ 3) MODEL, OPTIMIZER, SCHEDULER, CRITERION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2Model(\n",
    "    vocab_size=config[\"vocab_size\"],\n",
    "    block_size=config[\"block_size\"],\n",
    "    d_model=config[\"d_model\"],\n",
    "    n_heads=config[\"n_heads\"],\n",
    "    n_layers=config[\"n_layers\"],\n",
    "    dropout=config[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=1, verbose=True\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# â”€â”€â”€ 4) DATA LOADERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_dataset = TokenChunkDataset(train_inputs, train_targets)\n",
    "val_dataset   = TokenChunkDataset(val_inputs,   val_targets)\n",
    "train_loader  = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=config[\"batch_size\"], shuffle=True\n",
    ")\n",
    "val_loader    = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=config[\"batch_size\"], shuffle=False\n",
    ")\n",
    "\n",
    "# â”€â”€â”€ 5) OPTIONAL: RESUME FROM CHECKPOINT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "start_epoch = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "stop_counter = 0\n",
    "\n",
    "latest_ckpt = os.path.join(config[\"checkpoint_dir\"], \"best.pt\")\n",
    "if os.path.isfile(latest_ckpt):\n",
    "    checkpoint = torch.load(latest_ckpt, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "    print(f\"Resumed from epoch {start_epoch}, best_val_loss={best_val_loss:.4f}\")\n",
    "\n",
    "# â”€â”€â”€ 6) TRAINING + VALIDATION LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "    # â€” Training â€”\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [train]\")\n",
    "    for xb, yb in pbar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)  # (B, T, V)\n",
    "        B, T, V = logits.size()\n",
    "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix(train_loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # â€” Validation â€”\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch+1} [val]\"):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            B, T, V = logits.size()\n",
    "            loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # â€” Scheduler step & early stopping logic â€”\n",
    "    scheduler.step(avg_val_loss)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        stop_counter = 0\n",
    "        # Save best checkpoint\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }, latest_ckpt)\n",
    "        print(f\"New best model at epoch {epoch+1}, val_loss={avg_val_loss:.4f}\")\n",
    "    else:\n",
    "        stop_counter += 1\n",
    "        print(f\"No improvement for {stop_counter} epoch(s). Best={best_val_loss:.4f}\")\n",
    "        if stop_counter >= config[\"patience\"]:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # â€” Logging to wandb â€”\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "    })\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy7aAjoclU89"
   },
   "source": [
    "## multiâ€‘GPU training\n",
    "\n",
    "Hereâ€™s the simplest way to get multiâ€‘GPU training going in your notebook using PyTorchâ€™s DataParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xb27u9gDllR3"
   },
   "source": [
    "1. Wrap Your Model in DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_w1h9valVxm"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2Model(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# multiâ€‘GPU wrapper\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr8ziWHvmBZF"
   },
   "source": [
    "2. Adjust Checkpointing & Resume\n",
    "\n",
    "When saving or loading state dicts, remember that under DataParallel, your modelâ€™s weights live in model.module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12SplMQNmBxv"
   },
   "outputs": [],
   "source": [
    "# Saving\n",
    "to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
    "torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state_dict\": to_save.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"best_val_loss\": best_val_loss,\n",
    "    # Add other states like scheduler state if needed\n",
    "}, latest_ckpt)\n",
    "\n",
    "# Loading\n",
    "chk = torch.load(latest_ckpt)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.module.load_state_dict(chk[\"model_state_dict\"])\n",
    "else:\n",
    "    model.load_state_dict(chk[\"model_state_dict\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzEWe_y2mu7b"
   },
   "source": [
    "3. No Changes Needed to DataLoader\n",
    "\n",
    "DataParallel automatically splits each incoming batch (xb, yb) along the batch dimension and collects the outputs. You do not need to change how you construct your DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LV7I4BFmwqF"
   },
   "source": [
    "4. Full Example Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiV78WlBmva5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# â”€â”€â”€ 1) CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "config = {\n",
    "    \"vocab_size\":     vocab_size,\n",
    "    \"block_size\":     block_size,\n",
    "    \"d_model\":        d_model,\n",
    "    \"n_heads\":        n_heads,\n",
    "    \"n_layers\":       n_layers,\n",
    "    \"dropout\":        dropout_rate,\n",
    "    \"learning_rate\":  3e-4,\n",
    "    \"batch_size\":     32,\n",
    "    \"epochs\":         20,\n",
    "    \"patience\":       3,\n",
    "    \"checkpoint_dir\": \"./checkpoints\",\n",
    "}\n",
    "os.makedirs(config[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€ 2) MODEL + MULTIâ€‘GPU WRAP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2Model(\n",
    "    vocab_size=config[\"vocab_size\"],\n",
    "    block_size=config[\"block_size\"],\n",
    "    d_model=config[\"d_model\"],\n",
    "    n_heads=config[\"n_heads\"],\n",
    "    n_layers=config[\"n_layers\"],\n",
    "    dropout=config[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# â”€â”€â”€ 3) OPTIMIZER, SCHEDULER, CRITERION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=1, verbose=True\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# â”€â”€â”€ 4) DATA LOADERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_dataset = TokenChunkDataset(train_inputs, train_targets)\n",
    "val_dataset   = TokenChunkDataset(val_inputs,   val_targets)\n",
    "train_loader  = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=config[\"batch_size\"], shuffle=True\n",
    ")\n",
    "val_loader    = torch.utils.data.DataLoader(\n",
    "    val_dataset,   batch_size=config[\"batch_size\"], shuffle=False\n",
    ")\n",
    "\n",
    "# â”€â”€â”€ 5) OPTIONAL: RESUME FROM CHECKPOINT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "start_epoch = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "stop_counter = 0\n",
    "ckpt_path = os.path.join(config[\"checkpoint_dir\"], \"best.pt\")\n",
    "\n",
    "if os.path.isfile(ckpt_path):\n",
    "    chk = torch.load(ckpt_path, map_location=device)\n",
    "    # If wrapped in DataParallel, state dict under .module\n",
    "    target = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    target.load_state_dict(chk[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(chk[\"optimizer_state_dict\"])\n",
    "    start_epoch = chk[\"epoch\"] + 1\n",
    "    best_val_loss = chk[\"best_val_loss\"]\n",
    "    print(f\"Resumed from epoch {start_epoch}, best_val_loss={best_val_loss:.4f}\")\n",
    "\n",
    "# â”€â”€â”€ 6) TRAIN + VALIDATE LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "    # â€” Training â€”\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch+1} [train]\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(xb)                     # (B, T, V)\n",
    "        B, T, V = logits.size()\n",
    "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    avg_train = train_loss / len(train_loader)\n",
    "\n",
    "    # â€” Validation â€”\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch+1} [val]\"):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            B, T, V = logits.size()\n",
    "            loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
    "            val_loss += loss.item()\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "\n",
    "    # â€” Scheduler, Checkpointing, Early Stopping â€”\n",
    "    scheduler.step(avg_val)\n",
    "    if avg_val < best_val_loss:\n",
    "        best_val_loss = avg_val\n",
    "        stop_counter = 0\n",
    "        # Save best\n",
    "        to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": to_save.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }, ckpt_path)\n",
    "        print(f\"[Epoch {epoch+1}] New best val loss: {avg_val:.4f}\")\n",
    "    else:\n",
    "        stop_counter += 1\n",
    "        print(f\"[Epoch {epoch+1}] No improvement ({stop_counter}/{config['patience']})\")\n",
    "        if stop_counter >= config[\"patience\"]:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train: {avg_train:.4f} | Val: {avg_val:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH-2LQP0odWc"
   },
   "source": [
    "# âœ… Build the GPT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXf8h8U_oe4j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTConfig:\n",
    "    \"\"\"Configuration for the GPT model.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        block_size: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.d_model    = d_model\n",
    "        self.n_heads    = n_heads\n",
    "        self.n_layers   = n_layers\n",
    "        self.dropout    = dropout\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPTâ€2â€“style model:\n",
    "      â€¢ Token + Positional Embeddings\n",
    "      â€¢ N Transformer blocks (pre-LN, causal self-attn, FFN)\n",
    "      â€¢ Final LayerNorm\n",
    "      â€¢ Tied LM Head\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # 1) Embeddings\n",
    "        self.token_embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embed   = nn.Embedding(config.block_size, config.d_model)\n",
    "        self.dropout     = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 2) Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=config.d_model,\n",
    "                n_heads=config.n_heads,\n",
    "                block_size=config.block_size,\n",
    "                dropout=config.dropout\n",
    "            )\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "\n",
    "        # 3) Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        # 4) Languageâ€‘model head (tied to token embeddings)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        # tie weights\n",
    "        self.lm_head.weight = self.token_embed.weight\n",
    "\n",
    "        # ensure everything is initialized properly\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # GPTâ€‘style initialization (following OpenAI GPT-2)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.zeros_(module.bias)\n",
    "                nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, idx: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Input:\n",
    "          idx: (B, T) token IDs\n",
    "        Output:\n",
    "          logits: (B, T, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, \\\n",
    "            f\"Sequence length {T} exceeds block_size {self.config.block_size}\"\n",
    "\n",
    "        # Embeddings\n",
    "        token_embeddings = self.token_embed(idx)               # (B, T, d_model)\n",
    "        positions = torch.arange(T, device=idx.device)        # (T,)\n",
    "        pos_embeddings = self.pos_embed(positions)            # (T, d_model)\n",
    "        x = token_embeddings + pos_embeddings.unsqueeze(0)    # (B, T, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)                                       # (B, T, d_model)\n",
    "\n",
    "        # Final norm\n",
    "        x = self.ln_f(x)                                       # (B, T, d_model)\n",
    "\n",
    "        # Language modeling head\n",
    "        logits = self.lm_head(x)                               # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6FtbBwdokdd"
   },
   "source": [
    "How to instantiate and use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0AWiqt1okxD"
   },
   "outputs": [],
   "source": [
    "# 1) Create a config\n",
    "cfg = GPTConfig(\n",
    "    vocab_size=50257,\n",
    "    block_size=128,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    # dropout=0.1\n",
    ")\n",
    "\n",
    "# 2) Instantiate the model\n",
    "model = GPT(cfg).to(device)\n",
    "\n",
    "# 3) Forward pass\n",
    "input_ids = torch.randint(0, cfg.vocab_size, (2, 64), device=device)  # batch_size=2, seq_len=64\n",
    "logits = model(input_ids)  # â†’ (2, 64, cfg.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f985896"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Re-defining TokenPositionalEmbedding\n",
    "class TokenPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed   = nn.Embedding(block_size, d_model)\n",
    "        self.dropout     = nn.Dropout(0.1)  # GPT-2 uses dropout after embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()  # (batch_size, sequence_length)\n",
    "        # sanity check\n",
    "        assert T <= self.pos_embed.num_embeddings, (\n",
    "            f\"Sequence length T={T} exceeds block_size={self.pos_embed.num_embeddings}\"\n",
    "        )\n",
    "        tok_emb = self.token_embed(x)                    # (B, T, d_model)\n",
    "        pos_ids = torch.arange(T, device=x.device)       # (T,)\n",
    "        pos_emb = self.pos_embed(pos_ids)[None, :, :]    # (1, T, d_model)\n",
    "        out = tok_emb + pos_emb                          # (B, T, d_model)\n",
    "        return self.dropout(out)\n",
    "\n",
    "# Re-defining CausalSelfAttention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        # Output projection (projects concatenated heads back to d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # The mask will be created in forward based on input shape\n",
    "        # self.register_buffer(\n",
    "        #     \"mask\",\n",
    "        #     torch.tril(torch.ones((1, 1, 512, 512), dtype=torch.bool)),  # 512 = max block_size; adjust if you use different\n",
    "        #     persistent=False\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, d_model)  batch of embeddings\n",
    "        returns: (B, T, d_model) same shape, after self-attention\n",
    "        \"\"\"\n",
    "        B, T, C = x.size() # C = d_model\n",
    "        assert C == self.n_heads * self.d_head\n",
    "\n",
    "        # 1. project to queries, keys, values and reshape for multi-head\n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # (B, nh, T, dh)\n",
    "        k = self.k_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # (B, nh, T, dh)\n",
    "        v = self.v_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # (B, nh, T, dh)\n",
    "\n",
    "        # 2. compute scaled dot-product attention scores\n",
    "        #    q @ k^T : (B, nh, T, dh) @ (B, nh, dh, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "\n",
    "        # 3. apply causal mask: prevent attending to future positions\n",
    "        #    Create mask based on current sequence length T\n",
    "        causal_mask = torch.tril(torch.ones((T, T), device=x.device, dtype=torch.bool))[None, None, :, :] # (1, 1, T, T)\n",
    "        att = att.masked_fill(~causal_mask, float('-inf'))\n",
    "\n",
    "        # 4. softmax and dropout\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        # 5. attention output weighted sum\n",
    "        #    (B, nh, T, T) @ (B, nh, T, dh) -> (B, nh, T, dh)\n",
    "        out = att @ v\n",
    "\n",
    "        # 6. combine heads and final projection\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # (B, T, d_model)\n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "# Re-defining FeedForward\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Re-defining TransformerBlock\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, dropout_rate=0.1): # Corrected to accept dropout_rate\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, dropout_rate) # Pass dropout_rate\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, dropout_rate) # Pass dropout_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Re-defining GPTConfig\n",
    "class GPTConfig:\n",
    "    \"\"\"Configuration for the GPT model.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        block_size: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.d_model    = d_model\n",
    "        self.n_heads    = n_heads\n",
    "        self.n_layers   = n_layers\n",
    "        self.dropout    = dropout\n",
    "\n",
    "# Re-defining GPT\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPTâ€2â€“style model:\n",
    "      â€¢ Token + Positional Embeddings\n",
    "      â€¢ N Transformer blocks (pre-LN, causal self-attn, FFN)\n",
    "      â€¢ Final LayerNorm\n",
    "      â€¢ Tied LM Head\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # 1) Embeddings\n",
    "        self.token_embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embed   = nn.Embedding(config.block_size, config.d_model)\n",
    "        self.dropout     = nn.Dropout(config.dropout)\n",
    "\n",
    "        # self.token_pos_embed = TokenPositionalEmbedding(vocab_size, d_model, block_size)\n",
    "\n",
    "        # 2) Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=config.d_model,\n",
    "                n_heads=config.n_heads,\n",
    "                block_size=config.block_size,\n",
    "                dropout_rate=config.dropout # Pass dropout_rate here\n",
    "            )\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "\n",
    "        # 3) Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        # 4) Languageâ€‘model head (tied to token embeddings)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        # tie weights\n",
    "        self.lm_head.weight = self.token_embed.weight\n",
    "\n",
    "        # ensure everything is initialized properly\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.zeros_(module.bias)\n",
    "                nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, idx: torch.LongTensor) -> torch.FloatTensor:\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, \\\n",
    "            f\"Sequence length {T} exceeds block_size {self.config.block_size}\"\n",
    "\n",
    "        token_embeddings = self.token_embed(idx)\n",
    "        positions = torch.arange(T, device=idx.device)\n",
    "        pos_embeddings = self.pos_embed(positions)\n",
    "        x = token_embeddings + pos_embeddings.unsqueeze(0)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.config.block_size:] # Use config.block_size\n",
    "            logits = self(idx_cond)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "# --- Model Instantiation ---\n",
    "# Make sure device is defined (e.g., device = 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the configuration parameters (these must match the saved model)\n",
    "vocab_size   = 50257\n",
    "block_size   = 128\n",
    "d_model      = 256\n",
    "n_heads      = 4\n",
    "n_layers     = 4\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create a GPTConfig instance\n",
    "cfg = GPTConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout_rate\n",
    ")\n",
    "\n",
    "# Instantiate the GPT model\n",
    "model = GPT(cfg).to(device)\n",
    "\n",
    "print(\"GPT model instantiated successfully.\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87de0aea"
   },
   "source": [
    "To swap in your new `GPT` class for `GPT2Model`, you just need to:\n",
    "\n",
    "1.  Import/define the `GPTConfig` and `GPT` classes in your notebook (instead of `GPT2Model`).\n",
    "2.  Instantiate using `cfg = GPTConfig(...)` and `model = GPT(cfg)` instead of the old call.\n",
    "\n",
    "Everything else stays the sameâ€”your training loop, data loaders, optimizer, etc., all work unchanged because `GPT` and `GPT2Model` have the same `forward(idx) â†’ logits` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGREMn0Dp2J1"
   },
   "source": [
    "If youâ€™re using multiâ€‘GPU wrapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbpInzOvo--j"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a27fda2"
   },
   "source": [
    "Here is the code cell to **Load the Checkpoint**.\n",
    "\n",
    "This assumes your checkpoint file (`babygpt2_model.pt`) contains the `model_state_dict`, `optimizer_state_dict`, and `epoch`. Adjust the `checkpoint_path` if your file has a different name or location.\n",
    "\n",
    "Make sure your `GPTConfig`, `GPT`, and `AdamW` (optimizer) are defined in the notebook before running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a568735"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim # Needed to instantiate the optimizer before loading its state\n",
    "\n",
    "# Make sure your model configuration (cfg) and model (loaded_model) are instantiated first\n",
    "# You'll need the same architecture as when the checkpoint was saved.\n",
    "# Define the configuration parameters (these must match the saved model)\n",
    "vocab_size   = 50257\n",
    "block_size   = 128\n",
    "d_model      = 256\n",
    "n_heads      = 4\n",
    "n_layers     = 4\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create a GPTConfig instance\n",
    "cfg = GPTConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout_rate\n",
    ")\n",
    "loaded_model = GPT(cfg).to(device)\n",
    "\n",
    "# Instantiate the optimizer (must be the same type as when saved)\n",
    "# The learning rate will be overwritten when loading the state_dict, but you need an optimizer object first.\n",
    "# Use a dummy learning rate here, it will be replaced.\n",
    "optimizer = optim.AdamW(loaded_model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize variables for resuming\n",
    "start_epoch = 0\n",
    "# Initialize other variables you might have saved, like best_val_loss\n",
    "# best_val_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-532l7z2NbdG"
   },
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "import torch.serialization\n",
    "\n",
    "# Add GPTConfig to allowed globals for safe loading\n",
    "torch.serialization.add_safe_globals([GPTConfig])\n",
    "\n",
    "# Define the path to your comprehensive checkpoint file\n",
    "checkpoint_path = \"babygpt2_model_final_checkpoint.pt\"\n",
    "# checkpoint_path = \"babygpt2_model_final.pt\"\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device) # Add map_location=device if needed\n",
    "\n",
    "    # Load model state\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Load other states if saved\n",
    "    start_epoch = checkpoint['epoch'] + 1 # Start from the next epoch\n",
    "    # If you saved best_val_loss:\n",
    "    # best_val_loss = checkpoint['best_val_loss']\n",
    "    # If you saved scheduler state:\n",
    "    # scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "\n",
    "    print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "    # If you loaded best_val_loss:\n",
    "    # print(f\"Previous best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Checkpoint file not found at {checkpoint_path}. Please check the path.\")\n",
    "    # If no checkpoint is found, you might want to start training from epoch 0\n",
    "    # and initialize optimizer/scheduler here if they weren't initialized before the try block.\n",
    "\n",
    "except KeyError as e:\n",
    "     print(f\"Error loading checkpoint: Missing key {e}. Make sure the checkpoint dictionary structure matches.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the checkpoint: {e}\")\n",
    "\n",
    "\n",
    "# Move the loaded model and optimizer to the desired device (CPU or GPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loaded_model.to(device)\n",
    "# Optimizer doesn't typically need to be moved to device explicitly, but its internal tensors are\n",
    "# managed by the model's parameters being on the device.\n",
    "\n",
    "print(f\"Model and optimizer loaded, ready to resume training on {device}.\")\n",
    "\n",
    "# Now you can use the 'loaded_model' for inference or other tasks.\n",
    "# For example, to use it for text generation, you would need your tokenizer ('enc').\n",
    "prompt = \"Hello, how are you?\"\n",
    "token_ids = enc.encode(prompt)\n",
    "input_tensor = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "generated_output = loaded_model.generate(input_tensor, max_new_tokens=100)\n",
    "generated_text = enc.decode(generated_output[0].tolist())\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzSlffIF0KiU"
   },
   "outputs": [],
   "source": [
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70d2aa48"
   },
   "source": [
    "Here is the code cell for the **Training Loop**.\n",
    "\n",
    "This loop will start from the `start_epoch` loaded from the checkpoint and continue for the specified number of `num_epochs`.\n",
    "\n",
    "Make sure your `train_loader`, `val_loader`, and `criterion` are defined before running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f9825ae"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm # Assuming you want progress bars\n",
    "\n",
    "# Ensure model, optimizer, start_epoch, train_loader, val_loader, criterion, and device are defined\n",
    "# (These should be ready from the previous loading cell and your data loading cells)\n",
    "# Initialize variables for resuming\n",
    "start_epoch = 0\n",
    "\n",
    "num_epochs = 10 # Define the total number of epochs you want to train for (e.g., 10 total epochs)\n",
    "                # If you loaded from epoch 5 and num_epochs is 10, it will train for epochs 5, 6, 7, 8, 9.\n",
    "\n",
    "# --- Start or resume the training loop ---\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Use tqdm for a progress bar for the training loader\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\")\n",
    "\n",
    "    for x, y in train_loop:\n",
    "        x = x.to(device)  # Move input to the same device as the model\n",
    "        y = y.to(device)  # Move target to the same device as the model\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # Calculate loss\n",
    "        # Reshape logits and y to (batch_size * sequence_length, vocab_size) and (batch_size * sequence_length,)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update model parameters\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Optional: Add validation loop here ---\n",
    "    # It's good practice to evaluate on the validation set periodically\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad(): # Disable gradient calculation for validation\n",
    "       val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\")\n",
    "       for x, y in val_loop:\n",
    "           x = x.to(device)\n",
    "           y = y.to(device)\n",
    "           logits = model(x)\n",
    "           loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "           total_val_loss += loss.item()\n",
    "           val_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Optional: Save checkpoint after each epoch or periodically\n",
    "    # This is where you would save a new checkpoint to be able to resume again later\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.item(), # Or avg_train_loss\n",
    "        # Save other metrics if you want to resume early stopping\n",
    "        # 'best_val_loss': best_val_loss,\n",
    "    }, f'gpt2_checkpoint_epoch_{epoch+1}.pt')\n",
    "\n",
    "    # Optional: Save the final model or a final comprehensive checkpoint after training is complete\n",
    "    # Example saving just the model state:\n",
    "    torch.save(model.state_dict(), \"babygpt2_model_final.pt\")\n",
    "\n",
    "    # Example saving a final comprehensive checkpoint:\n",
    "    final_checkpoint = {\n",
    "        'epoch': num_epochs,  # Save the current epoch\n",
    "        'model_state_dict': model.state_dict(), # Save the model's weights and biases\n",
    "        'optimizer_state_dict': optimizer.state_dict(), # Save the optimizer's state\n",
    "        'loss': loss.item(), # Save the loss at this point (or average loss)\n",
    "        'config': cfg, # Uncomment if 'config' is accessible and you want to save it\n",
    "        # Optional:\n",
    "        # 'scheduler_state_dict': scheduler.state_dict(), # Save the scheduler's state\n",
    "        # 'best_val_loss': best_val_loss, # Save the best validation loss\n",
    "    }\n",
    "    torch.save(final_checkpoint, \"babygpt2_model_final_checkpoint.pt\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# # Optional: Save the final model or a final comprehensive checkpoint after training is complete\n",
    "# # Example saving just the model state:\n",
    "# torch.save(model.state_dict(), \"babygpt2_model_final.pt\")\n",
    "\n",
    "# # Example saving a final comprehensive checkpoint:\n",
    "# final_checkpoint = {\n",
    "#     'epoch': num_epochs,  # Save the current epoch\n",
    "#     'model_state_dict': model.state_dict(), # Save the model's weights and biases\n",
    "#     'optimizer_state_dict': optimizer.state_dict(), # Save the optimizer's state\n",
    "#     'loss': loss.item(), # Save the loss at this point (or average loss)\n",
    "#     'config': cfg, # Uncomment if 'config' is accessible and you want to save it\n",
    "#     # Optional:\n",
    "#     # 'scheduler_state_dict': scheduler.state_dict(), # Save the scheduler's state\n",
    "#     # 'best_val_loss': best_val_loss, # Save the best validation loss\n",
    "# }\n",
    "# torch.save(final_checkpoint, \"babygpt2_model_final_checkpoint.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKhIoNYImxD4"
   },
   "outputs": [],
   "source": [
    "# Optional: Save the final model or a final comprehensive checkpoint after training is complete\n",
    "# Example saving just the model state:\n",
    "torch.save(model.state_dict(), \"babygpt2_model_final.pt\")\n",
    "\n",
    "# Example saving a final comprehensive checkpoint:\n",
    "final_checkpoint = {\n",
    "    'epoch': num_epochs,  # Save the current epoch\n",
    "    'model_state_dict': model.state_dict(), # Save the model's weights and biases\n",
    "    'optimizer_state_dict': optimizer.state_dict(), # Save the optimizer's state\n",
    "    'loss': loss.item(), # Save the loss at this point (or average loss)\n",
    "    'config': cfg, # Uncomment if 'config' is accessible and you want to save it\n",
    "    # Optional:\n",
    "    # 'scheduler_state_dict': scheduler.state_dict(), # Save the scheduler's state\n",
    "    # 'best_val_loss': best_val_loss, # Save the best validation loss\n",
    "}\n",
    "torch.save(final_checkpoint, \"babygpt2_model_final_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGMWQTcZ28A0"
   },
   "source": [
    "# there are 2 ways to save the model\n",
    "\n",
    "1. `torch.save(model.state_dict(), \"babygpt2_model_final.pt\")`\n",
    "\n",
    "2. `torch.save(final_checkpoint, \"babygpt2_model_final_checkpoint.pt\")\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fc46116"
   },
   "source": [
    "### 1. Loading from `babygpt2_model_final.pt` (Model State Dictionary only)\n",
    "\n",
    "This method is suitable if you only need the model's learned weights for inference or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c04ab16"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure GPTConfig and GPT classes are defined in your notebook\n",
    "# Example instantiation (must match the architecture saved):\n",
    "# vocab_size = 50257\n",
    "# block_size = 128\n",
    "# d_model = 256\n",
    "# n_heads = 4\n",
    "# n_layers = 4\n",
    "# dropout_rate = 0.1\n",
    "# cfg = GPTConfig(vocab_size, block_size, d_model, n_heads, n_layers, dropout_rate)\n",
    "# loaded_model_state_only = GPT(cfg)\n",
    "\n",
    "# Define the path to the saved model state dictionary file\n",
    "model_state_path = \"babygpt2_model_final.pt\"\n",
    "\n",
    "# Instantiate the model (you need the architecture defined first)\n",
    "# Assuming 'cfg' is already defined from your notebook's setup\n",
    "loaded_model_state_only = GPT(cfg)\n",
    "\n",
    "\n",
    "# Load the state dictionary into the instantiated model\n",
    "try:\n",
    "    loaded_model_state_only.load_state_dict(torch.load(model_state_path, map_location=device)) # Use map_location=device if needed\n",
    "    print(f\"Model state dictionary loaded successfully from {model_state_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model state file not found at {model_state_path}. Please check the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model state dictionary: {e}\")\n",
    "\n",
    "# Move the loaded model to the desired device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loaded_model_state_only.to(device)\n",
    "\n",
    "print(f\"Model loaded for inference/evaluation and moved to {device}.\")\n",
    "\n",
    "# You can now use loaded_model_state_only for inference\n",
    "# For example: loaded_model_state_only.generate(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J54_AuhEfvDg"
   },
   "outputs": [],
   "source": [
    "model = loaded_model_state_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "266f9420"
   },
   "source": [
    "### 2. Loading from `babygpt2_model_final_checkpoint.pt` (Comprehensive Checkpoint)\n",
    "\n",
    "This method is suitable if you need to resume training, as it includes the optimizer state, epoch, and potentially other training-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c8fc1f0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim # Needed to instantiate optimizer before loading state\n",
    "import torch.serialization # Import serialization module\n",
    "\n",
    "# Ensure GPTConfig and GPT classes are defined in your notebook\n",
    "# Example instantiation (must match the architecture saved):\n",
    "# vocab_size = 50257\n",
    "# block_size = 128\n",
    "# d_model = 256\n",
    "# n_heads = 4\n",
    "# n_layers = 4\n",
    "# dropout_rate = 0.1\n",
    "# cfg = GPTConfig(vocab_size, block_size, d_model, n_heads, n_layers, dropout_rate)\n",
    "# loaded_model_checkpoint = GPT(cfg)\n",
    "\n",
    "# Instantiate the model (you need the architecture defined first)\n",
    "# Assuming 'cfg' is already defined from your notebook's setup\n",
    "loaded_model_checkpoint = GPT(cfg)\n",
    "\n",
    "\n",
    "# Instantiate the optimizer (must be the same type as when saved)\n",
    "# A dummy learning rate is fine here as it will be overwritten by the loaded state\n",
    "optimizer_checkpoint = optim.AdamW(loaded_model_checkpoint.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "# Define the path to the comprehensive checkpoint file\n",
    "checkpoint_file_path = \"babygpt2_model_final_checkpoint.pt\"\n",
    "\n",
    "# Initialize variables for resuming (will be overwritten if checkpoint exists)\n",
    "start_epoch_checkpoint = 0\n",
    "# best_val_loss_checkpoint = float('inf') # Uncomment if you saved this\n",
    "\n",
    "# Add GPTConfig to allowed globals for safe loading\n",
    "torch.serialization.add_safe_globals([GPTConfig])\n",
    "\n",
    "\n",
    "# Load the checkpoint dictionary\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_file_path, map_location=device) # Use map_location=device if needed\n",
    "\n",
    "    # Load model state\n",
    "    loaded_model_checkpoint.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_checkpoint.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Load other states if saved\n",
    "    start_epoch_checkpoint = checkpoint['epoch'] + 1\n",
    "    # if 'best_val_loss' in checkpoint:\n",
    "    #     best_val_loss_checkpoint = checkpoint['best_val_loss']\n",
    "    # if 'scheduler_state_dict' in checkpoint and scheduler_checkpoint is not None:\n",
    "    #      scheduler_checkpoint.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    # if 'config' in checkpoint:\n",
    "    #      loaded_cfg = checkpoint['config'] # Load the saved config\n",
    "\n",
    "    print(f\"Checkpoint loaded successfully from {checkpoint_file_path}\")\n",
    "    print(f\"Ready to resume training from epoch {start_epoch_checkpoint}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Checkpoint file not found at {checkpoint_file_path}. Please check the path.\")\n",
    "    # If no checkpoint, training will start from epoch 0 with initialized model/optimizer\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"Error loading checkpoint: Missing key {e}. Make sure the checkpoint dictionary structure matches.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the checkpoint: {e}\")\n",
    "\n",
    "# Move the loaded model and optimizer to the desired device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loaded_model_checkpoint.to(device)\n",
    "# Optimizer's state tensors are moved implicitly with the model's parameters\n",
    "\n",
    "print(f\"Model and optimizer loaded from checkpoint and moved to {device}.\")\n",
    "\n",
    "# You can now use loaded_model_checkpoint and optimizer_checkpoint to resume training\n",
    "# The next training loop should start from start_epoch_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOclqjCrJnev/LW0D5d035x",
   "collapsed_sections": [
    "t_0yRGUHhhzE",
    "18f3f187",
    "Hy7aAjoclU89",
    "SPoIWEIbJNvP"
   ],
   "gpuType": "T4",
   "mount_file_id": "1vNEoY3HCo3g-RVZKp0oe9Wsp92-dgH1_",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
