{"cells":[{"cell_type":"markdown","metadata":{"id":"4aD62_rmT1Qm"},"source":["##tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5928,"status":"ok","timestamp":1751653250198,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"jilyVQO0HHBa","outputId":"1105e670-83f3-4c0c-945f-6ca6af4f2091"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n","Requirement already satisfied: regex\u003e=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests\u003e=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.26.0-\u003etiktoken) (3.4.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.26.0-\u003etiktoken) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.26.0-\u003etiktoken) (2.4.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.26.0-\u003etiktoken) (2025.6.15)\n"]}],"source":["pip install tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":178,"status":"ok","timestamp":1751653250340,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"JlsHSnDjHY78","outputId":"89110e02-927e-4edb-d008-4d2f05a98d7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[15339, 1917]\n","hello world\n"]}],"source":["import tiktoken\n","\n","enc = tiktoken.encoding_for_model(\"gpt-4\")\n","tokens = enc.encode(\"hello world\")\n","print(tokens)\n","print(enc.decode(tokens))\n"]},{"cell_type":"markdown","metadata":{"id":"BqrJmFYgPjvG"},"source":["##tokenizer"]},{"cell_type":"markdown","metadata":{"id":"El-81A0wUKPn"},"source":["### Phase 1: Prepare Your Data\n","\n","- Convert all your training text to **UTF-8 bytes**.\n","- Collect statistics over these byte sequences."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":378,"status":"ok","timestamp":1751653250719,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"kSPJCFt7L_aZ","outputId":"00cd9602-a93e-46d1-ee90-6567f3e24956"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2025-07-04 18:20:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘tiny_shakespeare.txt’\n","\n","tiny_shakespeare.tx 100%[===================\u003e]   1.06M  --.-KB/s    in 0.04s   \n","\n","2025-07-04 18:20:51 (28.0 MB/s) - ‘tiny_shakespeare.txt’ saved [1115394/1115394]\n","\n"]}],"source":["# In Colab\n","!wget -O tiny_shakespeare.txt \\\n","     https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"]},{"cell_type":"markdown","metadata":{"id":"a6lpeXwLPoAf"},"source":["1. Load and Read the File"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1751653250746,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"HsopM5-aOSds","outputId":"89587ade-f0a7-4441-f413-bd1dca7d2134"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 1,115,394 characters.\n"]}],"source":["# In a Colab code cell\n","import glob\n","\n","# Read the entire file into one big string\n","with open(\"tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n","    text_data = f.read()\n","print(f\"Loaded {len(text_data):,} characters.\")"]},{"cell_type":"markdown","metadata":{"id":"wAVex3mVPqW2"},"source":["2. Convert All Text to UTF‑8 Bytes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1751653250760,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"vKTIsexQPPpE","outputId":"8565e05b-69a6-4288-eb24-c400b09a4093"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total bytes: 1,115,394\n"]}],"source":["# Convert the string to raw bytes\n","byte_data = text_data.encode(\"utf-8\")\n","print(f\"Total bytes: {len(byte_data):,}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1751653250855,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"2d4ead22","outputId":"957e74a8-2d51-4a4b-f52e-14926d9fccd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["First 100 characters of text_data:\n","First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n","--------------------------------------\n","\n","First 100 bytes of byte_data:\n","b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n"]}],"source":["print(\"First 100 characters of text_data:\")\n","print(text_data[:100])\n","print(\"--------------------------------------\")\n","print(\"\\nFirst 100 bytes of byte_data:\")\n","print(byte_data[:100])"]},{"cell_type":"markdown","metadata":{"id":"G1JQ5sdUTozk"},"source":["If you want to see the integer values of each byte, you can do:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1751653250855,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"OjezlnXRTsHV","outputId":"0fcebf1b-be0b-4f7d-b085-8881ffe5e86b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 58, 10, 66, 101, 102, 111, 114]\n"]}],"source":["print(list(byte_data[:20]))\n","# e.g. [70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, ...]\n"]},{"cell_type":"markdown","metadata":{"id":"UOy2Yl30TmEu"},"source":["If you want to see them as actual characters, you’d have to decode each byte back to a one‐character string (which may raise errors for non‑ASCII bytes):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1751653250856,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"VVgvhH1MTm7F","outputId":"33ee9c94-e234-4461-fec7-10d0a6c7f5c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r']\n"]}],"source":["chars = [bytes([b]).decode(\"utf-8\", \"replace\") for b in byte_data[:20]]\n","print(chars)\n"]},{"cell_type":"markdown","metadata":{"id":"qlWHmbJcQe6m"},"source":["3. Count Byte Frequencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1751653250861,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"Nb5CwCGGQFLJ","outputId":"cd083519-6ecd-42ed-8815-7141c316068f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Most common bytes:\n","  Byte 32 (' '): 169,892 occurrences\n","  Byte 101 ('e'): 94,611 occurrences\n","  Byte 116 ('t'): 67,009 occurrences\n","  Byte 111 ('o'): 65,798 occurrences\n","  Byte 97 ('a'): 55,507 occurrences\n","  Byte 104 ('h'): 51,310 occurrences\n","  Byte 115 ('s'): 49,696 occurrences\n","  Byte 114 ('r'): 48,889 occurrences\n","  Byte 110 ('n'): 48,529 occurrences\n","  Byte 105 ('i'): 45,537 occurrences\n"]}],"source":["from collections import Counter\n","\n","byte_freqs = Counter(byte_data)\n","print(\"Most common bytes:\")\n","for byte, freq in byte_freqs.most_common(10):\n","    char = bytes([byte]).decode(\"utf-8\", \"replace\")\n","    print(f\"  Byte {byte!r} ({char!r}): {freq:,} occurrences\")"]},{"cell_type":"markdown","metadata":{"id":"kg9HA8keQ6Md"},"source":["4. Inspect the Distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1751653250888,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"LCt8SfdXQhoK","outputId":"bfa4ef4f-ac0e-45f7-b92a-00fd79373c33"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Top byte frequencies (% of total):\n","  ' ' (byte 32): 15.23%\n","  'e' (byte 101): 8.48%\n","  't' (byte 116): 6.01%\n","  'o' (byte 111): 5.90%\n","  'a' (byte 97): 4.98%\n","  'h' (byte 104): 4.60%\n","  's' (byte 115): 4.46%\n","  'r' (byte 114): 4.38%\n","  'n' (byte 110): 4.35%\n","  'i' (byte 105): 4.08%\n"]}],"source":["total = sum(byte_freqs.values())\n","print(\"\\nTop byte frequencies (% of total):\")\n","for byte, freq in byte_freqs.most_common(10):\n","    pct = freq / total * 100\n","    char = bytes([byte]).decode(\"utf-8\", \"replace\")\n","    print(f\"  {char!r} (byte {byte}): {pct:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"5h0f0QsEU2Jo"},"source":["### Phase 2: Pre-tokenization (optional)\n","\n","- Use regex to chunk text (OpenAI uses this but GPT-2 doesn’t).\n","- **Or** just split into raw bytes (pure byte-level BPE)."]},{"cell_type":"markdown","metadata":{"id":"HoBPCiYuV4w8"},"source":["Whitespace \u0026 Punctuation Splitting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7031,"status":"ok","timestamp":1751653257921,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"LcGUQe6uQ8rB","outputId":"3e509d39-9e2b-4613-bc96-31f2d79e93a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n"]}],"source":["!pip install regex\n","import regex as re\n","\n","def pre_tokenize(text):\n","    text = text.lower()\n","    tokens = re.findall(\n","        r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\",\n","        text\n","    )\n","    return tokens\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Drn4-KQVmo9k"},"outputs":[],"source":["chunks = pre_tokenize(text_data)\n","byte_chunks = [chunk.encode('utf-8') for chunk in chunks]\n","byte_data = b''.join(byte_chunks)\n"]},{"cell_type":"markdown","metadata":{"id":"4En-CLOMV9wO"},"source":["Normalization (Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjS3gfU_VEe9"},"outputs":[],"source":["import unicodedata\n","norm_text = unicodedata.normalize('NFKC', text_data.lower())\n"]},{"cell_type":"markdown","metadata":{"id":"k_Wu4Q-fWDMV"},"source":["Chunk → Bytes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1XEEqxg8tjeLgZaO5MAgdt1WMOXVLoTmW"},"executionInfo":{"elapsed":3339,"status":"ok","timestamp":1751653323354,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"bUg3fH7-V_n6","outputId":"7f8f2e2b-2669-4b2d-bc64-0fbb5d7620c1"},"outputs":[],"source":["byte_sequences = [chunk.encode('utf-8') for chunk in chunks]\n","print(byte_sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1751653340033,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"MP_iWuHRWFm_"},"outputs":[{"name":"stdout","output_type":"stream","text":["b'first'\n","b' citizen'\n","b':'\n","b'\\n'\n","b'before'\n","b' we'\n","b' proceed'\n","b' any'\n","b' further'\n","b','\n"]}],"source":["# Inspect a few\n","for seq in byte_sequences[:10]:\n","    print(seq)"]},{"cell_type":"markdown","metadata":{"id":"VPpfDl1CXZ2M"},"source":["### Phase 3: Learn BPE Merges\n","\n","1. Start with a vocabulary of **single bytes** (0–255).\n","2. Count all **adjacent pairs** of tokens in the dataset.\n","3. Find the most frequent pair and merge it.\n","4. Add the merged pair to the vocabulary.\n","5. Repeat until you hit the target vocab size."]},{"cell_type":"markdown","metadata":{"id":"o1MTQMkodmLF"},"source":["Step 1: Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0D6Kcsl7dr5d"},"outputs":[],"source":["from collections import Counter\n","\n","tokens = list(byte_data)  # Start with raw bytes\n","vocab = {i: bytes([i]) for i in range(256)}  # Initial vocab\n","next_token_id = 256  # First new token ID\n","target_vocab_size = 50257  # Final vocab size\n"]},{"cell_type":"markdown","metadata":{"id":"cpu7eDMTdxoG"},"source":["Step 2: Count Adjacent Pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":505,"status":"ok","timestamp":1751653347510,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"B2FzE_3odzV0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pair (101,32) → ('e',' '): 27,965 occurrences\n","Pair (116,104) → ('t','h'): 26,047 occurrences\n","Pair (32,116) → (' ','t'): 24,243 occurrences\n","Pair (104,101) → ('h','e'): 19,268 occurrences\n","Pair (116,32) → ('t',' '): 16,508 occurrences\n","Pair (115,32) → ('s',' '): 15,486 occurrences\n","Pair (100,32) → ('d',' '): 14,542 occurrences\n","Pair (44,32) → (',',' '): 14,098 occurrences\n","Pair (32,97) → (' ','a'): 13,939 occurrences\n","Pair (111,117) → ('o','u'): 13,078 occurrences\n"]}],"source":["pair_freqs = Counter()\n","for i in range(len(tokens) - 1):\n","    pair = (tokens[i], tokens[i+1])\n","    pair_freqs[pair] += 1\n","\n","# Show the top 10 pairs\n","for (b1, b2), freq in pair_freqs.most_common(10):\n","    c1 = bytes([b1]).decode('utf-8', 'replace')\n","    c2 = bytes([b2]).decode('utf-8', 'replace')\n","    print(f\"Pair ({b1},{b2}) → ('{c1}','{c2}'): {freq:,} occurrences\")"]},{"cell_type":"markdown","metadata":{"id":"pUr64WPEeKvv"},"source":["Step 3: Merge Most Frequent Pair"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Iz_Xf7MceMy1"},"outputs":[],"source":["if not pair_freqs:\n","    print(\"No more pairs to merge.\")\n","else:\n","    most_common_pair, freq = pair_freqs.most_common(1)[0]\n","    b1, b2 = most_common_pair\n","\n","    new_token = next_token_id\n","    vocab[new_token] = vocab[b1] + vocab[b2]\n","    next_token_id += 1\n"]},{"cell_type":"markdown","metadata":{"id":"0UDdYj0keZeV"},"source":["Step 4: Replace Pair in Tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"j8W6c_6Hebe1"},"outputs":[],"source":["new_tokens = []\n","i = 0\n","while i \u003c len(tokens):\n","    if i \u003c len(tokens) - 1 and (tokens[i], tokens[i+1]) == most_common_pair:\n","        new_tokens.append(new_token)\n","        i += 2\n","    else:\n","        new_tokens.append(tokens[i])\n","        i += 1\n","tokens = new_tokens\n"]},{"cell_type":"markdown","metadata":{"id":"o0dEJVtQcRXW"},"source":["#### Wrap in a Loop to Build Full Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"elapsed":3293354,"status":"error","timestamp":1751656656780,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"WPa1FOXobAT3","outputId":"aed7da4b-8ae2-4dc3-b685-a8cd438a2f76"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocab size: 1000 | Token count: 410177\n","Vocab size: 2000 | Token count: 331700\n","Vocab size: 3000 | Token count: 296971\n","Vocab size: 4000 | Token count: 276174\n","Vocab size: 5000 | Token count: 261540\n","Vocab size: 6000 | Token count: 250416\n","Vocab size: 7000 | Token count: 241612\n","Vocab size: 8000 | Token count: 234259\n","Vocab size: 9000 | Token count: 228056\n","Vocab size: 10000 | Token count: 222734\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-21-473159464.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 14\u001b[0;31m         \u001b[0mpair_freqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpair_freqs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from collections import Counter\n","\n","tokens = list(byte_data)\n","vocab = {i: bytes([i]) for i in range(256)}\n","merges = {}\n","next_token_id = 256\n","target_vocab_size = 50257\n","\n","while len(vocab) \u003c target_vocab_size:\n","    # Step 1: Count adjacent token pairs\n","    pair_freqs = Counter()\n","    for i in range(len(tokens) - 1):\n","        pair = (tokens[i], tokens[i+1])\n","        pair_freqs[pair] += 1\n","\n","    if not pair_freqs:\n","        print(\"No more pairs to merge.\")\n","        break\n","\n","    # Step 2: Find most frequent pair\n","    most_common_pair, freq = pair_freqs.most_common(1)[0]\n","    b1, b2 = most_common_pair\n","\n","    # Step 3: Merge and assign new token ID\n","    new_token = next_token_id\n","    vocab[new_token] = vocab[b1] + vocab[b2]\n","    merges[(b1, b2)] = new_token\n","    next_token_id += 1\n","\n","    # Step 4: Replace in token stream\n","    new_tokens = []\n","    i = 0\n","    while i \u003c len(tokens):\n","        if i \u003c len(tokens) - 1 and (tokens[i], tokens[i+1]) == most_common_pair:\n","            new_tokens.append(new_token)\n","            i += 2\n","        else:\n","            new_tokens.append(tokens[i])\n","            i += 1\n","    tokens = new_tokens\n","\n","    if next_token_id % 1000 == 0:\n","        print(f\"Vocab size: {len(vocab)} | Token count: {len(tokens)}\")\n"]},{"cell_type":"markdown","metadata":{"id":"FUG1I5qsgSwM"},"source":["Add \u003c|endoftext|\u003e to Your Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHVS_fDjrvFO"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"rZoAnlmsgXel"},"source":["Step 1: Choose a Token ID"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iD-g2H5TgUzl"},"outputs":[],"source":["END_OF_TEXT_TOKEN = \"\u003c|endoftext|\u003e\"\n","eot_token_id = 50256  # always the last ID in GPT-2\n"]},{"cell_type":"markdown","metadata":{"id":"rPhYHRjkgZgr"},"source":["Step 2: Add It to Your Vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abN-smm6gb5k"},"outputs":[],"source":["vocab[eot_token_id] = END_OF_TEXT_TOKEN.encode(\"utf-8\")\n"]},{"cell_type":"markdown","metadata":{"id":"nnHIzEP_ghO8"},"source":["Bonus: Add Reverse Mapping (ID → Text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_F59mrdrgioG"},"outputs":[],"source":["id_to_token = {i: token for i, token in vocab.items()}\n"]},{"cell_type":"markdown","metadata":{"id":"lClulOStkzeU"},"source":["### Phase 4: Tokenization Function\n","\n","- Write a `tokenize(text) or encode(text)` function:\n","    - Convert text → UTF-8 bytes.\n","    - Apply your BPE merges greedily.\n","    - Output final token list."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qunBuS19lM0t"},"outputs":[],"source":["def encode(text, vocab=vocab, merges=merges, special_tokens=None):\n","    if special_tokens and text in special_tokens:\n","        return [special_tokens[text]]\n","\n","    tokens = []\n","    chunks = pre_tokenize(text)\n","\n","    for chunk in chunks:\n","        byte_seq = list(chunk.encode('utf-8'))\n","        ids = byte_seq[:]\n","\n","        # Apply greedy BPE on this chunk\n","        while True:\n","            pair_freqs = {}\n","            # Count frequencies of adjacent pairs\n","            current_pair_freqs = Counter()\n","            for i in range(len(ids) - 1):\n","                pair = (ids[i], ids[i+1])\n","                current_pair_freqs[pair] += 1\n","\n","            # Find the most frequent pair that has a merge rule\n","            best_pair = None\n","            max_freq = -1\n","            for pair, freq in current_pair_freqs.items():\n","                if pair in merges and freq \u003e max_freq:\n","                    max_freq = freq\n","                    best_pair = pair\n","\n","            if not best_pair:\n","                break # No more pairs to merge based on learned merges\n","\n","            # Merge the best pair\n","            new_id = merges[best_pair]\n","            new_ids = []\n","            i = 0\n","            while i \u003c len(ids):\n","                if i \u003c len(ids) - 1 and (ids[i], ids[i+1]) == best_pair:\n","                    new_ids.append(new_id)\n","                    i += 2\n","                else:\n","                    new_ids.append(ids[i])\n","                    i += 1\n","            ids = new_ids\n","\n","        tokens.extend(ids)\n","\n","    return tokens"]},{"cell_type":"markdown","metadata":{"id":"5kZRfdGjk1-T"},"source":["### Phase 5: Decoding Function\n","\n","- Map tokens → bytes → convert back to text (UTF-8 decode)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vr_vNvpFlS-s"},"outputs":[],"source":["def decode(tokens, vocab=vocab):\n","    \"\"\"\n","    Decodes a list of token IDs into a UTF-8 string.\n","\n","    Args:\n","        tokens (List[int]): List of token IDs.\n","        vocab (dict): Token ID → byte sequence.\n","\n","    Returns:\n","        str: The decoded string.\n","    \"\"\"\n","    byte_stream = b''.join([vocab[token] for token in tokens])\n","    return byte_stream.decode('utf-8', errors='replace')\n"]},{"cell_type":"markdown","metadata":{"id":"z2Rn5J7Eknq8"},"source":["# Test the Tokenizer End-to-End"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1751656907647,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"WBMU8O44ko6t","outputId":"a0dfcc35-02ff-42e9-9d17-5bd619931edb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original: Hello, world!\n","Tokens: [1359, 275, 111, 44, 3483, 114, 108, 100, 33]\n","Decoded: hello, world!\n"]}],"source":["text = \"Hello, world!\"\n","tokens = encode(text)\n","decoded = decode(tokens)\n","\n","print(\"Original:\", text)\n","print(\"Tokens:\", tokens)\n","print(\"Decoded:\", decoded)\n"]},{"cell_type":"markdown","metadata":{"id":"cJzYcRMAcbCG"},"source":["# Save the Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1751656665374,"user":{"displayName":"Kartik lodhi","userId":"14407704560312283465"},"user_tz":-330},"id":"5nJRuzIq3BSO","outputId":"0b126c93-635d-46d1-9d1a-e9efc8badd42"},"outputs":[{"name":"stdout","output_type":"stream","text":["merges_dict.json saved.\n","vocab.json saved.\n"]}],"source":["import json\n","\n","# Convert tuple keys in merges to strings for JSON serialization\n","merges_serializable = {str(k): v for k, v in merges.items()}\n","with open(\"merges_dict.json\", \"w\") as f:\n","    json.dump(merges_serializable, f)\n","print(\"merges_dict.json saved.\")\n","\n","# Convert byte values in vocab to list of integers for JSON serialization\n","vocab_serializable = {k: list(v) for k, v in vocab.items()}\n","with open(\"vocab.json\", \"w\") as f:\n","    json.dump(vocab_serializable, f)\n","print(\"vocab.json saved.\")"]},{"cell_type":"markdown","metadata":{"id":"6C8JosKX1GrK"},"source":["# Load the Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6665bcb"},"outputs":[],"source":["import json\n","\n","# Load vocabulary\n","with open(\"vocab.json\", \"r\") as f:\n","    loaded_vocab_str_keys = json.load(f)\n","    # Convert string keys back to integers and list values back to bytes\n","    loaded_vocab = {int(k): bytes(v) for k, v in loaded_vocab_str_keys.items()}\n","\n","print(f\"Loaded vocabulary with {len(loaded_vocab)} tokens.\")\n","\n","# Load merges\n","loaded_merges = {}\n","with open(\"merges.txt\", \"r\") as f:\n","    for line in f:\n","        b1_str, b2_str = line.strip().split()\n","        # Convert string representations back to integers\n","        b1, b2 = int(b1_str), int(b2_str)\n","        # The new token ID is assigned sequentially during the original merge process.\n","        # We need to reconstruct the merge rule (pair -\u003e new_id).\n","        # A simple way for this specific implementation is to find the new ID in the loaded vocab.\n","        # This assumes the merge file order matches the new token ID assignment order.\n","        # A more robust way would be to save the merge pair and its resulting new ID together.\n","        # For now, let's rebuild the merges dictionary based on the loaded vocab.\n","        # This part is tricky because merges.txt only stores the pairs, not the new ID.\n","        # A better saving format would be { \"(b1, b2)\": new_id }.\n","\n","# Let's rethink how to load merges. If merges.txt only contains pairs, we need to\n","# rebuild the merges dictionary based on the loaded vocab.\n","# A more standard way to save BPE merges is a list of pairs in the order they were merged.\n","# The new ID is implicitly assigned sequentially.\n","\n","# Let's assume merges.txt contains lines like \"b1 b2\" in the order they were merged.\n","# We can reconstruct the merges dictionary by replaying the merge process on a dummy sequence,\n","# or by saving the merges dictionary directly.\n","\n","# A more direct way to save and load merges is to save the dictionary itself.\n","# Let's assume you saved merges as a dictionary { (b1, b2): new_id }\n","# If you saved it with tuple keys, you might need to convert keys to strings like \"(b1, b2)\"\n","# and convert them back on loading.\n","\n","# Let's modify the saving code to save the merges dictionary in a loadable format.\n","# And then provide the loading code for that format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9197d824"},"outputs":[],"source":["import json\n","\n","# Let's assume you saved merges like this:\n","# with open(\"merges_dict.json\", \"w\") as f:\n","#     # Convert tuple keys to strings\n","#     merges_serializable = {str(k): v for k, v in merges.items()}\n","#     json.dump(merges_serializable, f)\n","\n","# Loading the merges dictionary assuming it was saved as a JSON with string keys \"(b1, b2)\"\n","try:\n","    with open(\"merges_dict.json\", \"r\") as f:\n","        loaded_merges_str_keys = json.load(f)\n","        # Convert string keys \"(b1, b2)\" back to tuple (int, int)\n","        loaded_merges = {}\n","        for key_str, new_id in loaded_merges_str_keys.items():\n","            # Assuming key_str is like \"(b1, b2)\"\n","            b1_str, b2_str = key_str.strip(\"()\").split(\", \")\n","            loaded_merges[(int(b1_str), int(b2_str))] = new_id\n","\n","    print(f\"Loaded {len(loaded_merges)} merge rules.\")\n","\n","except FileNotFoundError:\n","    print(\"merges_dict.json not found. Please ensure you have saved the merges dictionary.\")\n","    loaded_merges = {} # Initialize as empty if file not found\n","\n","# Now you can use loaded_vocab and loaded_merges to continue training or tokenize.\n","\n","# Example of using the loaded tokenizer (assuming you have encode and decode functions)\n","# text_to_tokenize = \"This is a test sentence.\"\n","# tokens = encode(text_to_tokenize, loaded_vocab, loaded_merges)\n","# decoded_text = decode(tokens, loaded_vocab)\n","# print(\"Original:\", text_to_tokenize)\n","# print(\"Tokens:\", tokens)\n","# print(\"Decoded:\", decoded_text)"]},{"cell_type":"markdown","metadata":{"id":"8909dbb2"},"source":["### Steps to Further Train the Tokenizer with New Data\n","\n","To incrementally train your tokenizer with additional data, you need to load the current state of your vocabulary and merge rules and then continue the BPE training process with the new data.\n","\n","Here are the general steps:\n","\n","1.  **Load Existing Tokenizer State:**\n","    *   Load the saved `vocab` dictionary (from `vocab.json`).\n","    *   Load the saved `merges` dictionary (from `merges_dict.json`).\n","\n","2.  **Prepare New Data:**\n","    *   Load your new text data.\n","    *   Combine the new data with your existing training data if you want to find merges across the entire corpus, or process the new data separately depending on your strategy.\n","    *   Convert the new or combined text data to UTF-8 bytes.\n","    *   Apply your pre-tokenization function to the new or combined byte data to get the initial list of tokens (integer IDs).\n","\n","3.  **Continue BPE Training:**\n","    *   Initialize the BPE training loop (similar to the code in cell `WPa1FOXobAT3`) using the `loaded_vocab` and `loaded_merges`.\n","    *   Set `next_token_id` to `max(loaded_vocab.keys()) + 1`.\n","    *   Use the pre-tokenized list from the new or combined data as the starting `tokens` for the loop.\n","    *   The loop will continue to find the most frequent pairs in the current `tokens` and add new merges to your `merges` dictionary and new tokens to your `vocab` dictionary until your desired vocabulary size is reached or another stopping criterion is met.\n","\n","4.  **Save Updated Tokenizer State:**\n","    *   After training, save the updated `vocab` and `merges` dictionaries back to their respective files to preserve the progress.\n","\n","By following these steps, you can incrementally build and refine your tokenizer's vocabulary and merge rules as more data becomes available."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Slsg5aZgvmob"},"outputs":[],"source":["new_text_data = \"new text data to train your tokenizer further\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92dff1fa"},"outputs":[],"source":["# Step 1: Load Existing Tokenizer State\n","import json\n","\n","try:\n","    with open(\"vocab.json\", \"r\") as f:\n","        loaded_vocab_str_keys = json.load(f)\n","        loaded_vocab = {int(k): bytes(v) for k, v in loaded_vocab_str_keys.items()}\n","    print(f\"Loaded vocabulary with {len(loaded_vocab)} tokens.\")\n","\n","    with open(\"merges_dict.json\", \"r\") as f:\n","        loaded_merges_str_keys = json.load(f)\n","        loaded_merges = {}\n","        for key_str, new_id in loaded_merges_str_keys.items():\n","            b1_str, b2_str = key_str.strip(\"()\").split(\", \")\n","            loaded_merges[(int(b1_str), int(b2_str))] = new_id\n","    print(f\"Loaded {len(loaded_merges)} merge rules.\")\n","\n","except FileNotFoundError:\n","    print(\"Could not find vocab.json or merges_dict.json. Starting with a fresh tokenizer.\")\n","    loaded_vocab = {i: bytes([i]) for i in range(256)}\n","    loaded_merges = {}\n","\n","# Initialize vocab and merges with the loaded state\n","vocab = loaded_vocab\n","merges = loaded_merges\n","\n","# Step 2: Prepare New Data (and combine with old data for comprehensive training)\n","# Assuming text_data contains your original training data\n","# Assuming new_text_data contains your additional training data (from cell Slsg5aZgvmob)\n","combined_text_data = text_data + new_text_data\n","\n","# Convert combined text to UTF-8 bytes\n","combined_byte_data = combined_text_data.encode(\"utf-8\")\n","\n","# Apply pre-tokenization to the combined byte data\n","# This gives us the initial sequence of tokens (byte IDs and potentially pre-tokenized chunk IDs)\n","# We need to make sure pre_tokenize handles byte sequences correctly, or apply it to text first\n","# Let's re-apply pre_tokenize to the combined text and then convert to byte sequences\n","combined_chunks = pre_tokenize(combined_text_data)\n","initial_tokens = []\n","for chunk in combined_chunks:\n","    byte_seq = list(chunk.encode('utf-8'))\n","    # At this stage, we treat these as initial token IDs (bytes) before applying merges\n","    initial_tokens.extend(byte_seq)\n","\n","# Initialize tokens for the BPE loop with the initial tokens from combined data\n","tokens = initial_tokens\n","\n","# Step 3: Continue BPE Training\n","# Set next_token_id to continue from the last learned token ID\n","if vocab:\n","    next_token_id = max(vocab.keys()) + 1\n","else:\n","    next_token_id = 256 # Should not happen if loading base vocab\n","\n","# Define the target vocabulary size (can be larger than before if desired)\n","# Let's continue towards the original target size if not reached, or set a new target\n","current_vocab_size = len(vocab)\n","print(f\"Starting training from vocab size: {current_vocab_size}\")\n","\n","# Continue merging until the target_vocab_size is reached or no more merges are possible\n","# We need to make sure the target_vocab_size variable is available or define it again\n","# Let's use the original target_vocab_size from cell WPa1FOXobAT3 if it exists, otherwise define it.\n","# Assuming target_vocab_size is available in the environment\n","# If not, you might need to define it here: target_vocab_size = 50257\n","\n","while len(vocab) \u003c target_vocab_size:\n","    # Step 1: Count adjacent token pairs in the current 'tokens' stream\n","    pair_freqs = Counter()\n","    for i in range(len(tokens) - 1):\n","        pair = (tokens[i], tokens[i+1])\n","        pair_freqs[pair] += 1\n","\n","    # Step 2: Find most frequent pair that is not already a single token in the vocab\n","    # and has a merge rule defined or will be a new merge\n","    # We need to find the most frequent pair that *can* be merged\n","    best_pair = None\n","    max_freq = -1\n","    for pair, freq in pair_freqs.items():\n","        # Check if the pair is not already a single token (shouldn't happen with how tokens are generated)\n","        # and if its frequency is higher than the current max\n","        if freq \u003e max_freq:\n","             # Also ensure the pair consists of valid token IDs currently in our vocabulary (or are base bytes)\n","             # This check is implicitly handled by how 'tokens' is constructed and updated\n","             max_freq = freq\n","             best_pair = pair\n","\n","    if not best_pair or max_freq \u003c 2: # Stop if no pairs or only single occurrences\n","        print(\"No more pairs to merge with frequency \u003e= 2.\")\n","        break\n","\n","    # Step 3: Merge and assign new token ID if this pair hasn't been merged before\n","    if best_pair not in merges:\n","        new_token = next_token_id\n","        vocab[new_token] = vocab[best_pair[0]] + vocab[best_pair[1]]\n","        merges[best_pair] = new_token\n","        next_token_id += 1\n","    else:\n","         # This pair has been merged before, get the existing new token ID\n","         new_token = merges[best_pair]\n","\n","\n","    # Step 4: Replace in token stream\n","    new_tokens = []\n","    i = 0\n","    while i \u003c len(tokens):\n","        if i \u003c len(tokens) - 1 and (tokens[i], tokens[i+1]) == best_pair:\n","            new_tokens.append(new_token)\n","            i += 2\n","        else:\n","            new_tokens.append(tokens[i])\n","            i += 1\n","    tokens = new_tokens\n","\n","    if len(vocab) % 1000 == 0:\n","        print(f\"Vocab size: {len(vocab):,} | Token count: {len(tokens):,}\")\n","    if len(vocab) \u003e= target_vocab_size:\n","         print(f\"Reached target vocab size: {target_vocab_size:,}\")\n","         break\n","\n","\n","print(f\"Final vocab size after training: {len(vocab):,}\")\n","print(f\"Final token count after training: {len(tokens):,}\")\n","\n","\n","# Step 4: Save Updated Tokenizer State\n","# Convert tuple keys in merges to strings for JSON serialization\n","merges_serializable = {str(k): v for k, v in merges.items()}\n","with open(\"merges_dict.json\", \"w\") as f:\n","    json.dump(merges_serializable, f)\n","print(\"Updated merges_dict.json saved.\")\n","\n","# Convert byte values in vocab to list of integers for JSON serialization\n","vocab_serializable = {k: list(v) for k, v in vocab.items()}\n","with open(\"vocab.json\", \"w\") as f:\n","    json.dump(vocab_serializable, f)\n","print(\"Updated vocab.json saved.\")"]},{"cell_type":"markdown","metadata":{"id":"10GXF_MvzPXH"},"source":["# problems"]},{"cell_type":"markdown","metadata":{"id":"FooPBIgizei7"},"source":["While there isn't one specific, commonly used technical term just for this particular problem in BPE training (where newly frequent pairs from new data might be missed if the vocabulary size limit is already met), it relates to the broader challenges in machine learning when dealing with:\n","\n","Incremental Learning / Online Learning: Training a model (or a component like a tokenizer) on new data over time.\n","Data Drift: When the distribution or characteristics of incoming data change compared to the data the system was initially trained on. In your case, the new data has different frequency patterns.\n","Fixed Vocabulary Limitations: The inherent limitation of tokenizers with a predefined maximum vocabulary size, where less frequent or newly appearing patterns must be represented by sequences of existing tokens rather than being merged into new, more efficient tokens.\n","You could describe it as a challenge of \"vocabulary adaptation in incremental BPE training under data drift\" or \"handling emergent frequent patterns with a fixed-size BPE vocabulary\".\n","\n","If you want to revisit this, you can simply refer back to the discussion about incremental tokenizer training and the issue of increasing the target vocabulary size when new data introduces new frequent patterns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McxPuBc7zfEZ"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMp8hW5L8rS2KqPYrksuPQB","collapsed_sections":["4aD62_rmT1Qm"],"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}